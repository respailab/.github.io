---
---

@string{aps = {American Physical Society,}}

@article{tarun2023fast,
  abbr      = {IEEE},
  title     = {Fast yet effective machine unlearning},
  author    = {Tarun, Ayush K and Chundawat, Vikram S and Mandal, Murari and Kankanhalli, Mohan},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  year      = {2023},
  publisher = {IEEE},
  pdf       = {https://arxiv.org/pdf/2111.08947.pdf},
  preview   = {fast-unlearning.png},
  selected  = {true}
}

@article{10097553,
  abbr     = {IEEE},
  author   = {Chundawat, Vikram S. and Tarun, Ayush K. and Mandal, Murari and Kankanhalli, Mohan},
  journal  = {IEEE Transactions on Information Forensics and Security},
  title    = {Zero-Shot Machine Unlearning},
  year     = {2023},
  volume   = {18},
  number   = {},
  pages    = {2345-2354},
  keywords = {Data models;Training;Data privacy;Training data;Computational modeling;Regulation;Machine learning;Machine unlearning;machine learning security and privacy;data privacy},
  doi      = {10.1109/TIFS.2023.3265506},
  pdf      = {https://arxiv.org/pdf/2201.05629.pdf},
  preview  = {zero-shot-unlearning.png},
  selected = {true}
}

@inproceedings{pmlr-v202-tarun23a,
  title     = {Deep Regression Unlearning},
  author    = {Tarun, Ayush Kumar and Chundawat, Vikram Singh and Mandal, Murari and Kankanhalli, Mohan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages     = {33921--33939},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume    = {202},
  series    = {Proceedings of Machine Learning Research},
  month     = {23--29 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v202/tarun23a/tarun23a.pdf},
  url       = {https://proceedings.mlr.press/v202/tarun23a.html},
  preview   = {deep-regression-unlearning.png},
  abstract  = {With the introduction of data protection and privacy regulations, it has become crucial to remove the lineage of data on demand from a machine learning (ML) model. In the last few years, there have been notable developments in machine unlearning to remove the information of certain training data efficiently and effectively from ML models. In this work, we explore unlearning for the regression problem, particularly in deep learning models. Unlearning in classification and simple linear regression has been considerably investigated. However, unlearning in deep regression models largely remains an untouched problem till now. In this work, we introduce deep regression unlearning methods that generalize well and are robust to privacy attacks. We propose the Blindspot unlearning method which uses a novel weight optimization process. A randomly initialized model, partially exposed to the retain samples and a copy of the original model are used together to selectively imprint knowledge about the data that we wish to keep and scrub off the information of the data we wish to forget. We also propose a Gaussian fine tuning method for regression unlearning. The existing unlearning metrics for classification are not directly applicable to regression unlearning. Therefore, we adapt these metrics for the regression setting. We conduct regression unlearning experiments for computer vision, natural language processing and forecasting applications. Our methods show excellent performance for all these datasets across all the metrics. Source code: https://github.com/ayu987/deep-regression-unlearning}
}

@misc{tarun2024ecoval,
  title         = {EcoVal: An Efficient Data Valuation Framework for Machine Learning},
  author        = {Ayush K Tarun and Vikram S Chundawat and Murari Mandal and Hong Ming Tan and Bowei Chen and Mohan Kankanhalli},
  year          = {2024},
  eprint        = {2402.09288},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  pdf           = {https://arxiv.org/pdf/2402.09288},
  preview       = {ecoval.png},
  abstract      = {Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \textit{production function}, a concept which is popularly used to estimate the amount of output based on factors like labor and capital in a traditional free economic market. We provide a formal proof of our valuation technique and elucidate the principles and mechanisms that enable its accelerated performance. We demonstrate the real-world applicability of our method by showcasing its effectiveness for both in-distribution and out-of-sample data. This work addresses one of the core challenges of efficient data valuation at scale in machine learning models.}
}

@misc{sinha2023distill,
  title         = {Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation},
  author        = {Yash Sinha and Murari Mandal and Mohan Kankanhalli},
  year          = {2023},
  eprint        = {2309.16173},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  preview       = {distil.png},
  pdf           = {https://arxiv.org/pdf/2309.16173}
}

@article{VERMA2023119957,
  title    = {Efficient neural architecture search for emotion recognition},
  journal  = {Expert Systems with Applications},
  volume   = {224},
  pages    = {119957},
  year     = {2023},
  issn     = {0957-4174},
  doi      = {https://doi.org/10.1016/j.eswa.2023.119957},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417423004591},
  author   = {Monu Verma and Murari Mandal and Satish Kumar Reddy and Yashwanth Reddy Meedimale and Santosh Kumar Vipparthi},
  keywords = {Human emotion, Micro-expression, Macro-expression, Neural architecture search (NAS), Deep learning},
  abstract = {Automated human emotion recognition from facial expressions is a well-studied problem and still remains a very challenging task. Some efficient or accurate deep learning models have been presented in the literature. However, it is quite difficult to design a model that is both efficient and accurate at the same time. Moreover, identifying the minute feature variations in facial regions for both macro and micro-expressions requires expertise in network design. In this paper, we proposed to search for a highly efficient and robust neural architecture for both macro and micro-level facial expression recognition. To the best of our knowledge, this is the first attempt to design a NAS-based solution for both macro and micro-expression recognition. We produce lightweight models with a gradient-based architecture search algorithm. To maintain consistency between macro and micro-expressions, we utilize dynamic imaging and convert micro-expression sequences into a single frame, preserving the spatiotemporal features in the facial regions. The EmoNAS has evaluated over 13 datasets (7 macro expression datasets: CK+, DISFA, MUG, ISED, OULU-VIS CASIA, FER2013, RAF-DB, and 6 micro-expression datasets: CASME-I, CASME-II, CAS(ME)2̂, SAMM, SMIC, MEGC2019 challenge). The proposed models outperform the existing state-of-the-art methods and perform very well in terms of speed and space complexity.}
}

@article{Chundawat_Tarun_Mandal_Kankanhalli_2023,
  title        = {Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks Using an Incompetent Teacher},
  volume       = {37},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/25879},
  doi          = {10.1609/aaai.v37i6.25879},
  abstractnote = {Machine unlearning has become an important area of research due to an increasing need for machine learning (ML) applications to comply with the emerging data privacy regulations. It facilitates the provision for removal of certain set or class of data from an already trained ML model without requiring retraining from scratch. Recently, several efforts have been put in to make unlearning to be effective and efficient. We propose a novel machine unlearning method by exploring the utility of competent and incompetent teachers in a student-teacher framework to induce forgetfulness. The knowledge from the competent and incompetent teachers is selectively transferred to the student to obtain a model that doesn’t contain any information about the forget data. We experimentally show that this method generalizes well, is fast and effective. Furthermore, we introduce the zero retrain forgetting (ZRF) metric to evaluate any unlearning method. Unlike the existing unlearning metrics, the ZRF score does not depend on the availability of the expensive retrained model. This makes it useful for analysis of the unlearned model after deployment as well. We present results of experiments conducted for random subset forgetting and class forgetting on various deep networks and across different application domains. Code is at: https://github.com/vikram2000b/bad-teaching- unlearning},
  number       = {6},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Chundawat, Vikram S and Tarun, Ayush K and Mandal, Murari and Kankanhalli, Mohan},
  year         = {2023},
  month        = {Jun.},
  pages        = {7210-7217}
}

@article{10021309,
  author   = {Goyal, Adit and Mandal, Murari and Hassija, Vikas and Aloqaily, Moayad and Chamola, Vinay},
  journal  = {IEEE Transactions on Computational Social Systems},
  title    = {Captionomaly: A Deep Learning Toolbox for Anomaly Captioning in Social Surveillance Systems},
  year     = {2024},
  volume   = {11},
  number   = {1},
  pages    = {207-215},
  keywords = {Anomaly detection;Task analysis;Video surveillance;Training;Deep learning;Computational modeling;Visualization;Anomaly detection;deep learning;surveillance;toolbox;video captioning;UCF-Crime},
  doi      = {10.1109/TCSS.2022.3230262}
}
