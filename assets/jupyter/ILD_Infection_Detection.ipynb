{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16258acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve, confusion_matrix\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032e426",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b770c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_CLASSES = 2\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "NUM_FOLDS = 10\n",
    "MODEL_SAVE_DIR = \"saved_models_10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f89d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.ImageFolder(root='../dataset', transform=transform)\n",
    "    return dataset\n",
    "\n",
    "# Function to define ResNet-50 model\n",
    "def create_resnet_model():\n",
    "    model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "    return model\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, roc_auc, pr_auc, cm, fpr, tpr\n",
    "\n",
    "# Function to plot ROC curve\n",
    "def plot_roc_curve(fpr, tpr, auc):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot Precision-Recall curve\n",
    "def plot_precision_recall_curve(recall, precision, auc):\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label='PR curve (area = %0.2f)' % auc)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54db2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to save models\n",
    "if not os.path.exists(MODEL_SAVE_DIR):\n",
    "    os.makedirs(MODEL_SAVE_DIR)\n",
    "\n",
    "# Load data\n",
    "dataset = load_data()\n",
    "\n",
    "# Initialize k-fold cross validation\n",
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802022ce",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f24f328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform k-fold cross validation\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(dataset):\n",
    "    fold += 1\n",
    "    print(f\"Fold {fold}...\")\n",
    "\n",
    "    # Create model\n",
    "    model = create_resnet_model()\n",
    "    model = model.cuda()\n",
    "\n",
    "    # Define data loaders\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_index)\n",
    "    test_sampler = torch.utils.data.SubsetRandomSampler(test_index)\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss}\")\n",
    "\n",
    "    # Evaluate model\n",
    "    accuracy, precision, recall, f1, roc_auc, pr_auc, cm, fpr, tpr = evaluate_model(model, test_loader)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), os.path.join(MODEL_SAVE_DIR, f\"model_fold{fold}.pth\"))\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plot_roc_curve(fpr, tpr, roc_auc)\n",
    "\n",
    "    # Plot Precision-Recall curve\n",
    "    plot_precision_recall_curve(recall, precision, pr_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba11bf3",
   "metadata": {},
   "source": [
    "## Testing patients folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d004766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 224\n",
    "MODEL_PATH = \"saved_models_10\"\n",
    "CLASS_NAMES = [\"infected\", \"not_infected\"]  # Replace with your class names\n",
    "ROOT_FOLDER = \"../CTIMGS\"\n",
    "# TEST_FOLDERS = [\"SHAKUNTALA_NAYAK\", \"SULOCHANA_DAS\", \"person1\", \"BHARATI_BAG\", \"SARADA_KANHAR\", \"SAROJ_KUMAR_BEHERA\", \"SUCHITRA_JENA\"]  # Replace with your test folders\n",
    "TEST_FOLDERS = os.listdir(ROOT_FOLDER)[:10]\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(m):\n",
    "    model = models.resnet50(weights=None)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(num_ftrs, len(CLASS_NAMES))\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_PATH, m)))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to preprocess image\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "# Function to predict class for an image\n",
    "def predict_image_class(image_path):\n",
    "    image = preprocess_image(image_path)\n",
    "    outputs = model(image)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    class_idx = predicted.item()\n",
    "    return CLASS_NAMES[class_idx]\n",
    "\n",
    "# Function to test a folder and print the results\n",
    "def test_folder(folder_path):\n",
    "    print(f\"Testing folder: {folder_path}\")\n",
    "    image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    # Predict class for each image in the folder\n",
    "    predicted_classes = []\n",
    "    for image_file in image_files:\n",
    "        predicted_class = predict_image_class(image_file)\n",
    "        predicted_classes.append(predicted_class)\n",
    "\n",
    "    # Count the number of predictions for each class\n",
    "    infected_count = predicted_classes.count(\"infected\")\n",
    "    not_infected_count = predicted_classes.count(\"not_infected\")\n",
    "    \n",
    "    print(\"Infected : \", infected_count, \"Not infected : \", not_infected_count)\n",
    "    \n",
    "    # Determine the class with the higher number of predictions\n",
    "    if infected_count > not_infected_count:\n",
    "        print(f\"The folder contains {infected_count} 'infected' images.\")\n",
    "    elif infected_count < not_infected_count:\n",
    "        print(f\"The folder contains {not_infected_count} 'not infected' images.\")\n",
    "    else:\n",
    "        print(\"The folder contains an equal number of 'infected' and 'not infected' images.\")\n",
    "    print()\n",
    "\n",
    "# Load the trained model onto GPU\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "saved_models = [\"model_fold1.pth\", \"model_fold2.pth\", \"model_fold3.pth\", \"model_fold4.pth\", \"model_fold5.pth\", \"model_fold6.pth\", \"model_fold7.pth\", \"model_fold8.pth\", \"model_fold9.pth\", \"model_fold10.pth\"]\n",
    "# saved_models = [\"model_fold5.pth\"]\n",
    "for m in saved_models:\n",
    "    model = load_model(m).to(device)\n",
    "\n",
    "    # Test each folder and display the results with progress status\n",
    "    for folder in TEST_FOLDERS:\n",
    "        test_folder(os.path.join(ROOT_FOLDER, folder))\n",
    "        print(\"----------------------------------------\")\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ae014",
   "metadata": {},
   "source": [
    "## Visualization using GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = 224\n",
    "MODEL_PATH = \"saved_models/model_fold1.pth\"\n",
    "CLASS_NAMES = [\"infected\", \"not_infected\"]\n",
    "ROOT_FOLDER = \"../CTIMGS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess image\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    return image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "def load_model():\n",
    "    model = models.resnet50(weights=None)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(num_ftrs, len(CLASS_NAMES))\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cuda:0')))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93305487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradcam(image_path, model, target_class):\n",
    "    # Load the image and preprocess it\n",
    "    image = preprocess_image(image_path)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(image)\n",
    "    \n",
    "    # Get the gradients of the target class output with respect to the feature map\n",
    "    model.zero_grad()\n",
    "    target_class_score = output[:, target_class]\n",
    "    target_class_score.backward()\n",
    "    \n",
    "    # Get the gradients from the last convolutional layer\n",
    "    gradients = model.conv1.weight.grad\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "    \n",
    "    # Get the activations of the last convolutional layer\n",
    "    activations = model.conv1(image).detach()\n",
    "    \n",
    "    # Weight the channels by corresponding gradients\n",
    "    for i in range(pooled_gradients.shape[0]):\n",
    "#         print(gradients.shape)\n",
    "#         print(activations.shape)\n",
    "#         print(pooled_gradients.shape)\n",
    "        activations[:, i, :, :] *= pooled_gradients[i]\n",
    "    \n",
    "    # Average the channels of the activations\n",
    "    heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "    \n",
    "    # Normalize the heatmap\n",
    "    heatmap = torch.clamp(heatmap, min=0)\n",
    "    heatmap /= torch.max(heatmap)\n",
    "    \n",
    "    # Convert heatmap to numpy array\n",
    "    heatmap = heatmap.cpu().numpy()\n",
    "    \n",
    "    # Resize heatmap to match the original image size\n",
    "    heatmap = cv2.resize(heatmap, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "    # Convert heatmap to RGB\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Load the original image\n",
    "    original_image = cv2.imread(image_path)\n",
    "    original_image = cv2.resize(original_image, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "    # Overlay heatmap on the original image\n",
    "    overlaid_image = cv2.addWeighted(original_image, 0.5, heatmap, 0.5, 0)\n",
    "    \n",
    "    return overlaid_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict class for an image\n",
    "def predict_image_class(image_path):\n",
    "    image = preprocess_image(image_path)\n",
    "    outputs = model(image)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    class_idx = predicted.item()\n",
    "    return CLASS_NAMES[class_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d8b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_folder(folder_path):\n",
    "    print(f\"Testing folder: {folder_path}\")\n",
    "    \n",
    "    image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    for image_file in image_files:\n",
    "        predicted_class = predict_image_class(image_file)\n",
    "        \n",
    "        # Generate Grad-CAM\n",
    "        gradcam = generate_gradcam(image_file, model, CLASS_NAMES.index(predicted_class))\n",
    "        output_path = os.path.splitext(image_file)[0] + \"_gradcam.jpg\"\n",
    "        save_dir = \"GradCAM/\"+output_path.split('/')[2]+'/'+output_path.split('/')[3]\n",
    "        cv2.imwrite(save_dir, gradcam)\n",
    "        print(save_dir, \"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e08b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model onto GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c1c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_FOLDERS = [\"person1\", \"person2\"]  # Replace with your test folders\n",
    "\n",
    "# Test each folder and display the results with Grad-CAMs\n",
    "for folder in TEST_FOLDERS:\n",
    "    try:\n",
    "        os.mkdir(\"GradCAM/\"+folder)\n",
    "    except:\n",
    "        print(\"File exists\")\n",
    "    test_folder(os.path.join(ROOT_FOLDER, folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea4e4c",
   "metadata": {},
   "source": [
    "### generating some visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def combine_images(image_folder1, image_folder2, output_folder):\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # List image files in both folders\n",
    "    images1 = sorted(os.listdir(image_folder1))\n",
    "    images2 = sorted(os.listdir(image_folder2))\n",
    "\n",
    "    # Iterate through images and combine them\n",
    "    for img1_name, img2_name in zip(images1, images2):\n",
    "        if img1_name.endswith(\".jpg\") and img2_name.endswith(\".jpg\"):\n",
    "            img1_path = os.path.join(image_folder1, img1_name)\n",
    "            img2_path = os.path.join(image_folder2, img2_name)\n",
    "            img1 = Image.open(img1_path)\n",
    "            img2 = Image.open(img2_path)\n",
    "            \n",
    "            # Combine images horizontally\n",
    "            combined_img = Image.new('RGB', (img1.width + img2.width, img1.height))\n",
    "            combined_img.paste(img1, (0, 0))\n",
    "            combined_img.paste(img2, (img1.width, 0))\n",
    "\n",
    "            # Save combined image to output folder\n",
    "            combined_img.save(os.path.join(output_folder, f\"{img1_name[:-4]}_combined.jpg\"))\n",
    "\n",
    "# Example usage:\n",
    "image_folder1 = \"../CTIMGS/person1\"\n",
    "image_folder2 = \"GradCAM/person1\"\n",
    "output_folder = \"combined_images_person1\"\n",
    "combine_images(image_folder1, image_folder2, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
